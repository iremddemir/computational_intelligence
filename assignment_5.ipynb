{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Neural Architecture Search (NAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, Neural Architecture Search (NAS) will be implemented. The goal of NAS is to find the best neural network architecture for a given task. The dataset that will be used in this assignment is SVHN (same as assignment 4). The possible configurations of the NN are given in the instructions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "EPS = 1.0e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab settings\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "results_dir = \"/content/gdrive/MyDrive/CI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same dataset code from assignment 4 will be used here. The dataset is already downloaded in the `data` folder. The dataset is loaded in the code below. Moreover, the neccessary code for carrying dataloaders to GPU is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import SVHN\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# svhn dataset from torchvision is used for effective data download\n",
    "SVHN(root =  \"/content/gdrive/MyDrive/CI\" ,split='test', download=True)\n",
    "SVHN(root =  \"/content/gdrive/MyDrive/CI\" ,split='train', download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HouseNumbers(Dataset):\n",
    "    \"\"\"House Numbers Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root = './', mode=\"train\", transform=None, val_ratio=0.2):\n",
    "        \"\"\"\n",
    "        Dataset Class for House Numbers Dataset. \n",
    "        It uses SVHN dataset from torchvision to download the dataset but only uses the train and test splits.\n",
    "        It creates its own data and target variables by splitting the train set into train and validation sets. \n",
    "        So that it can be used in a standard way for training, validation, and test purposes with PyTorch DataLoader.\n",
    "        Args:\n",
    "            root (string): Directory with all the images (default: './')\n",
    "            mode (string): train, val, or test (default: train)\n",
    "            transforms (function): Optional transform to be applied on a sample.\n",
    "            val_ratio (float): Ratio of validation set when mode is train (default: 0.2)\n",
    "        \"\"\"\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.val_ratio = val_ratio\n",
    "        self.root = root\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            # load test set\n",
    "            test_set = SVHN(root=self.root, split='test', transform=transforms.ToTensor())\n",
    "            # set data and target variables\n",
    "            self.data = test_set.data\n",
    "            self.target = test_set.labels\n",
    "        else:\n",
    "            # decide train and validation indices \n",
    "            # set random seed to 0 for achieving the same result in each instance of the class\n",
    "            train_indices, val_indices = train_test_split(\n",
    "                                            np.arange(len(SVHN(root=self.root, split='train'))), \n",
    "                                            test_size=self.val_ratio, \n",
    "                                            random_state=0)\n",
    "            # if mode is train, load train set\n",
    "            if self.mode == 'train':\n",
    "                complete = SVHN(root=self.root, split='train', transform=transforms.ToTensor())\n",
    "                self.data = torch.utils.data.Subset(complete.data, train_indices)\n",
    "                self.target = np.array(complete.labels)[train_indices]\n",
    "            # if mode is val, load validation set\n",
    "            elif self.mode == 'val':\n",
    "                complete = SVHN(root=self.root, split='train', transform=transforms.ToTensor())\n",
    "                self.data = torch.utils.data.Subset(complete.data, val_indices)\n",
    "                self.target = np.array(complete.labels)[val_indices]\n",
    "            else :\n",
    "                raise ValueError('Invalid mode %s' % self.mode)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample_x = self.data[idx]\n",
    "        sample_y = self.target[idx]\n",
    "        if self.transform:\n",
    "            sample_x = self.transform(sample_x)\n",
    "        return (sample_x, sample_y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training, validation and test sets.\n",
    "train_data = HouseNumbers(mode=\"train\", root= \"/content/gdrive/MyDrive/CI\")\n",
    "val_data = HouseNumbers(mode=\"val\",  root= \"/content/gdrive/MyDrive/CI\")\n",
    "test_data = HouseNumbers(mode=\"test\",  root= \"/content/gdrive/MyDrive/CI\")\n",
    "\n",
    "# Initialize data loaders.\n",
    "training_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move dataloaders to device as well\n",
    "# otherwise the training process will give an error\n",
    "# model parameters will be on the device and have type torch.cuda.FloatTensor\n",
    "# but the data will be on the CPU and have type torch.FloatTensor \n",
    "# so we need to move the data to the device that the model is on to make sure they are the same type\n",
    "# to fix this issue, found following solution in the pytorch documentation \n",
    "# modified it to fit our needs\n",
    "# ref: https://pytorch.org/tutorials/beginner/nn_tutorial.html#wrapping-dataloader\n",
    "class WrappedDataLoaderCustomize():\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        # in the original code, there is a function \n",
    "        # but here it will be fixed: 'to_device' so not included\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in self.dl:\n",
    "            yield self.move_to_device(i, self.device)\n",
    "    \n",
    "    def move_to_device(self, b, device):\n",
    "        # since the our data can be tuple shaped, we need to move each element to the device recursively\n",
    "        # check the input type\n",
    "        # move each element to the device recursively\n",
    "        if isinstance(b, (list, tuple)):\n",
    "            return [self.move_to_device(x, device) for x in b]\n",
    "        # move the input to the device directly\n",
    "        return b.to(device)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the implementation of a convolutional neural network of the following form:\n",
    "\n",
    "    Conv2d → f(.) → Pooling → Flatten → Linear 1 → f(.) → Linear 2 → Softmax \n",
    "    \n",
    "However, different choices in each building block are allowed as follows:\n",
    "\n",
    "● Conv2d:\n",
    "  - Number of filters: 8, 16, 32\n",
    "  - kernel=(3,3), stride=1, padding=1 OR kernel=(5,5), stride=1, padding=2\n",
    "  \n",
    "● f(.):\n",
    "  - ReLU OR sigmoid OR tanh OR softplus OR ELU\n",
    "\n",
    "● Pooling:\n",
    "  - 2x2 OR Identity\n",
    "  - Average OR Maximum\n",
    "\n",
    "● Linear 1:\n",
    "  - Number of neurons: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "\n",
    "Altogether, there are 4500 possible configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('comp411')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40dd6e9603a48eda36538272fd95ed3b36cd00a3b7df7195515789ffdf02208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
