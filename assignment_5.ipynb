{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Neural Architecture Search (NAS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, Neural Architecture Search (NAS) will be implemented. The goal of NAS is to find the best neural network architecture for a given task. The dataset that will be used in this assignment is Scikit Learn-Digits (same as assignment 4). The possible configurations of the NN are given in the instructions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "EPS = 1.0e-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab settings if code is run on Colab\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/gdrive\")\n",
    "results_dir = \"/content/gdrive/MyDrive/CI\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same dataset code from assignment 4 will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following class for Digits dataset is taken from Assignment-4 \n",
    "# This is a class for the dataset of small (8px x 8px) digits.\n",
    "class Digits(Dataset):\n",
    "    \"\"\"Scikit-Learn Digits dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, mode=\"train\", transforms=None):\n",
    "        digits = load_digits()\n",
    "        if mode == \"train\":\n",
    "            self.data = digits.data[:1000].astype(np.float32)\n",
    "            self.targets = digits.target[:1000]\n",
    "        elif mode == \"val\":\n",
    "            self.data = digits.data[1000:1350].astype(np.float32)\n",
    "            self.targets = digits.target[1000:1350]\n",
    "        else:\n",
    "            self.data = digits.data[1350:].astype(np.float32)\n",
    "            self.targets = digits.target[1350:]\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_x = self.data[idx]\n",
    "        sample_y = self.targets[idx]\n",
    "        if self.transforms:\n",
    "            sample_x = self.transforms(sample_x)\n",
    "        return (sample_x, sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training, validation and test sets.\n",
    "train_data = Digits(mode=\"train\")\n",
    "val_data = Digits(mode=\"val\")\n",
    "test_data = Digits(mode=\"test\")\n",
    "\n",
    "# Initialize data loaders.\n",
    "training_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section includes the implementation of a convolutional neural network of the following form:\n",
    "\n",
    "    Conv2d → f(.) → Pooling → Flatten → Linear 1 → f(.) → Linear 2 → Softmax \n",
    "    \n",
    "However, different choices in each building block are allowed as follows:\n",
    "\n",
    "● Conv2d:\n",
    "  - Number of filters: 8, 16, 32\n",
    "  - kernel=(3,3), stride=1, padding=1 OR kernel=(5,5), stride=1, padding=2\n",
    "  \n",
    "● f(.):\n",
    "  - ReLU OR sigmoid OR tanh OR softplus OR ELU\n",
    "\n",
    "● Pooling:\n",
    "  - 2x2 OR Identity\n",
    "  - Average OR Maximum\n",
    "\n",
    "● Linear 1:\n",
    "  - Number of neurons: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "\n",
    "Altogether, there are 1200 possible configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code is taken from Assignment-4 \n",
    "# This module reshapes an input (matrix -> tensor).\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.size = size  # a list\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == np.prod(self.size)\n",
    "        return x.view(x.shape[0], *self.size)\n",
    "\n",
    "\n",
    "# This module flattens an input (tensor -> matrix) by blending dimensions\n",
    "# beyond the batch size.\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], -1)\n",
    "\n",
    "def architecture_decoder(encoded_config, input_size=8, input_depth=1):\n",
    "    '''\n",
    "    This function accepts an encoded configuration and returns the corresponding architecture.\n",
    "    Args:\n",
    "        encoded_config (string): encoded configuration of 3 + 2 + 5 + 2 + 2 + 10 = 24 digits of 0 or 1\n",
    "        input_size (int): the size of a one dimension (width or height) of the input image\n",
    "        input_depth (int): the depth of the input image\n",
    "        - The input image is assumed to be square and have the same width and height. - \n",
    "        - Input of a NN will have size of input_size x input_size x input_depth -\n",
    "    Returns:\n",
    "        classnet (nn.Module): the corresponding architecture with the given encoded configuration\n",
    "        of     \n",
    "        Conv2d → f(.) → Pooling → Flatten → Linear 1 → f(.) → Linear 2 → Softmax \n",
    "    '''\n",
    "    # sanity checks on the input \n",
    "    #print('Given configuration: ', encoded_config)\n",
    "    assert len(encoded_config) == 24, \"encoded_config must be 24 digits\"\n",
    "    assert all([x in [0, 1] for x in encoded_config]), \"encoded_config must consits of 0 or 1\"\n",
    "    # divide the encoded_config into corresponding parts\n",
    "    conv1_filter_part = encoded_config[:3]\n",
    "    conv1_kernel_part = encoded_config[3:5]\n",
    "    f_part = encoded_config[5:10]\n",
    "    pooling_size_part = encoded_config[10:12]\n",
    "    pooling_method_part = encoded_config[12:14]\n",
    "    fc_part = encoded_config[14:]\n",
    "    # check there is only one 1 in each group of 3 + 2 + 5 + 2 + 2 + 10 \n",
    "    assert sum([int(x) for x in conv1_filter_part]) == 1, \"conv1_filter_part must have only one 1\"\n",
    "    assert sum([int(x) for x in conv1_kernel_part]) == 1, \"conv1_kernel_part must have only one 1\"\n",
    "    assert sum([int(x) for x in f_part]) == 1, \"f_part must have only one 1\"\n",
    "    assert sum([int(x) for x in pooling_size_part]) == 1, \"pooling_size_part must have only one 1\"\n",
    "    assert sum([int(x) for x in pooling_method_part]) == 1, \"pooling_method_part must have only one 1\"\n",
    "    assert sum([int(x) for x in fc_part]) == 1, \"fc_part must have only one 1\"\n",
    "    \n",
    "    # decode number of filters\n",
    "    filter_no = 8 if conv1_filter_part[0] == '1' else 16 if conv1_filter_part[1] == '1' else 32\n",
    "    # decode kernel size\n",
    "    kernel_size = 3 if conv1_kernel_part[0] == '1' else 5\n",
    "    padding = 1 if kernel_size == 3 else 2\n",
    "    stride = 1 \n",
    "    # decode f\n",
    "    f = nn.ReLU() if f_part[0] == '1' else nn.Sigmoid() if f_part[1] == '1' else nn.Tanh() if f_part[2] == '1' else nn.Softplus() if f_part[3] == '1' else nn.ELU()\n",
    "    # decode pooling size\n",
    "    pooling_size = 2 if pooling_size_part[0] == '1' else 1\n",
    "    # decode pooling method\n",
    "    pooling_method = nn.MaxPool2d if pooling_method_part[0] == '1' else nn.AvgPool2d\n",
    "    # decode fc\n",
    "    # Number of neurons: 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\n",
    "    neuron_no = 10 if fc_part[0] == '1' else 20 if fc_part[1] == '1' else 30 if fc_part[2] == '1' else 40 if fc_part[3] == '1' else 50 if fc_part[4] == '1' else 60 if fc_part[5] == '1' else 70 if fc_part[6] == '1' else 80 if fc_part[7] == '1' else 90 if fc_part[8] == '1' else 100\n",
    "\n",
    "    # calculate the input size of the first linear layer\n",
    "    # the input size of the first linear layer is the output size of the last pooling layer\n",
    "    # the output size of the last pooling layer is calculated as follows:\n",
    "    # output_size = (input_size - kernel_size + 2 * padding) / stride + 1\n",
    "    output_of_conv = (input_size - kernel_size + 2 * padding) / stride + 1\n",
    "    output_of_pooling = output_of_conv / pooling_size\n",
    "    output_of_flatten = output_of_pooling ** 2 * filter_no\n",
    "\n",
    "    # create the architecture\n",
    "    classnet = nn.Sequential(\n",
    "        # Reshape the input to a vector\n",
    "        Reshape(size=(input_depth, input_size, input_size)),\n",
    "        nn.Conv2d(input_depth, filter_no, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "        f,\n",
    "        pooling_method(pooling_size),\n",
    "        Flatten(),\n",
    "        nn.Linear(int(output_of_flatten), neuron_no),\n",
    "        f,\n",
    "        nn.Linear(neuron_no, 10),\n",
    "        nn.Softmax(dim=1)\n",
    "    )\n",
    "\n",
    "    # calculate number of weights \n",
    "    # to see details of calculation please see the cell below\n",
    "    number_of_weights = (kernel_size * kernel_size * input_depth + 1)* filter_no + ((output_of_flatten+1) * neuron_no) + ((neuron_no+1) * 10)\n",
    "    return classnet, number_of_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accesory Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of weights for the given architecture is:  206742\n"
     ]
    }
   ],
   "source": [
    "# Weights for different layers can be calculated as follows:\n",
    "# CONV = (KERNEL_SIZE**2 * INPUT_CHANNEL)* OUTPUT_CHANNEL\n",
    "# FC = INPUT_SIZE* OUTPUT_SIZE\n",
    "# Pooling, ReLU, Sigmoid, etc does not have any weights\n",
    "# Then we know that for same kernel size as the number of filters increases the number of weights are increased\n",
    "# Moreover, the number of filters are also effects the input size of the first linear layer thus as it increases the number of weights are increased\n",
    "# Also neuron number in the first linear layer should be maximized to increase the number of weights as it increases weights in first layer and higher input size for the second linear layer \n",
    "# For kernel size we know that as it increases the number of weights are increased however, it may also reduce the input size of the first linear\n",
    "# Thus we need to understand which one is more effective\n",
    "# For the given configs, output size of the conv layer stays the same for both cases as one is 3x3 with stride 1 pad 1 and other is 5x5 with stride 1 pad 2 \n",
    "# given the formulation of O = I - K + 2P / S + 1 we can observe they stay the same so both are equal \n",
    "# thus maximum number of weights are calculated as follows:\n",
    "# choose maximum kernel size, maximum neuron output size for first layer and also add the weights of the second linear layer\n",
    "# +1 represents the bias term\n",
    "Nmax = (5 * 5* 1 + 1 ) * 32 + ((64*32+1) * 100) + ((100 +1) * 10)\n",
    "print(\"Maximum number of weights for the given architecture is: \", Nmax)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def init_generation_creator(population_size, split_indexes,length):\n",
    "    '''\n",
    "    This function creates the initial generation of binary strings \n",
    "    to represent the initial population of the genetic algorithm.\n",
    "    Args:\n",
    "        population_size (int): the size of the population\n",
    "        split_indexes (list): the split points of the binary strings\n",
    "        in our case, the split points are [0, 3, 5, 10, 12, 14]\n",
    "        gene_length (list): the length of overall gen\n",
    "    Returns:\n",
    "        population (list): the initial population of the genetic algorithm\n",
    "    '''\n",
    "    population = []\n",
    "    for i in range(population_size):\n",
    "        ind = [0]*length\n",
    "        for j in range(len(split_indexes)-1):\n",
    "            #print('For indexes between {} and {}'.format(split_indexes[j],split_indexes[j+1]))\n",
    "            one_ind = np.random.choice(range(split_indexes[j],split_indexes[j+1]), size=1)\n",
    "            #print('Chosen index is: ', one_ind)\n",
    "            ind[int(one_ind)] = 1\n",
    "        #print('For indexes between {} and {}'.format(split_indexes[-1],length))\n",
    "        one_ind = np.random.choice(range(split_indexes[-1],length), size=1)\n",
    "        #print('Chosen index is: ', one_ind)\n",
    "        ind[int(one_ind)] = 1\n",
    "        population.append(ind)\n",
    "    return population\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following classifier code is taken from Assignment-4\n",
    "class ClassifierNeuralNet(nn.Module):\n",
    "    def __init__(self, classnet):\n",
    "        super(ClassifierNeuralNet, self).__init__()\n",
    "        # We provide a sequential module with layers and activations\n",
    "        self.classnet = classnet\n",
    "        # The loss function (given in the instruction)\n",
    "        self.nll = nn.CrossEntropyLoss()  # it requires log-softmax as input!!\n",
    "\n",
    "    # This function classifies an image x to a class.\n",
    "    # The output must be a class label (long).\n",
    "    def classify(self, x):\n",
    "        # ------\n",
    "        # PLEASE FILL IN\n",
    "        # calculate scores of given classnet\n",
    "        scores = self.classnet(x)\n",
    "        # predict the label based on highest score\n",
    "        y_pred = scores.argmax(dim=1)\n",
    "        # cast to long\n",
    "        y_pred = y_pred.long()\n",
    "        return y_pred\n",
    "\n",
    "    # This function is crucial for a module in PyTorch.\n",
    "    # In our framework, this class outputs a value of the loss function.\n",
    "    def forward(self, x, y, reduction=\"avg\"):\n",
    "        # ------\n",
    "        # PLEASE FILL IN\n",
    "        # nll between log-softmax result and the target\n",
    "        loss = self.nll(self.classnet(x), y)\n",
    "        # return the loss based on reduction\n",
    "        if reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss.mean()\n",
    "    \n",
    "    def objective(self, x, y, Nmax, lambda_ = 0.01):\n",
    "        # this is the objective function for assessing the quality of the model wrt Evalutionary Algorithm on validation set\n",
    "        # the function required to calculate is the following\n",
    "        '''\n",
    "        Modify the objective function (i.e., classification error, ClassError) by adding a\n",
    "    penalty for the number of weights in the CNN: Objective=ClassError+λ 𝑁𝑝 / 𝑁𝑚𝑎𝑥\n",
    "    where 𝑁𝑝 denotes the number of weights of a model and 𝑁𝑚𝑎𝑥 is a number of\n",
    "    weights of the largest possible neural network in the search space.\n",
    "    Take λ = 0. 01.\n",
    "        '''\n",
    "        # calculate the loss\n",
    "        loss = self.forward(x, y, reduction=\"sum\")\n",
    "        # calculate the number of parameters\n",
    "        Np = sum([p.numel() for p in self.parameters()])\n",
    "        # calculate the objective\n",
    "        objective = loss + lambda_ * Np / Nmax\n",
    "        return objective\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training procedure from Assignment 4 part-2 with minor changes\n",
    "# PLEASE DO NOT REMOVE!\n",
    "# The training procedure\n",
    "def training(\n",
    "    name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, verbose=False\n",
    "):\n",
    "    nll_val = []\n",
    "    error_val = []\n",
    "    best_nll = 1000.0\n",
    "    best_error_val = 1000.0\n",
    "    patience = 0\n",
    "    model_best_of_epochs = None\n",
    "    # Main training loop\n",
    "    for e in range(num_epochs):\n",
    "        model.train()  # set the model to the training mode\n",
    "        # load batches\n",
    "        for indx_batch, (batch, targets) in enumerate(training_loader):\n",
    "            # calculate the forward pass (loss function for given images and labels)\n",
    "            loss = model.forward(batch, targets)\n",
    "            # remember we need to zero gradients! Just in case!\n",
    "            optimizer.zero_grad()\n",
    "            # calculate backward pass\n",
    "            loss.backward(retain_graph=True)\n",
    "            # run the optimizer\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation: Evaluate the model on the validation data\n",
    "        loss_e, error_e = evaluation(val_loader, model_best=model, epoch=e, verbose=verbose)\n",
    "        nll_val.append(loss_e)  # save for plotting\n",
    "        error_val.append(error_e)  # save for plotting\n",
    "\n",
    "        # Early-stopping: update the best performing model and break training if no\n",
    "        # progress is observed.\n",
    "        if e == 0:\n",
    "            torch.save(model, name + \".model\")\n",
    "            best_nll = loss_e\n",
    "        else:\n",
    "            if loss_e < best_nll:\n",
    "                torch.save(model, name + \".model\")\n",
    "                best_nll = loss_e\n",
    "                patience = 0\n",
    "                model_best_of_epochs = model\n",
    "                best_error_val = error_e\n",
    "            else:\n",
    "                patience = patience + 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    # Return the best model and the corresponding classification error for this configuration\n",
    "    return model_best_of_epochs, best_error_val\n",
    "  \n",
    "# The evaluation procedure from Assignment 4 part-2 with minor changes\n",
    "# Moreover for evaluation algorithm the modified classification error is returned\n",
    "# PLEASE DO NOT REMOVE\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None, verbose=False):\n",
    "    # If available, load the best performing model\n",
    "    if model_best is None:\n",
    "        model_best = torch.load(name + \".model\")\n",
    "\n",
    "    model_best.eval()  # set the model to the evaluation mode\n",
    "    loss_test = 0.0\n",
    "    loss_error = 0.0\n",
    "    N = 0.0\n",
    "    # start evaluation\n",
    "    for indx_batch, (test_batch, test_targets) in enumerate(test_loader):\n",
    "        # loss (nll)\n",
    "        loss_test_batch = model_best.forward(test_batch, test_targets, reduction=\"sum\")\n",
    "        loss_test = loss_test + loss_test_batch.item()\n",
    "        # classification error\n",
    "        y_pred = model_best.classify(test_batch)\n",
    "        e = 1.0 * (y_pred == test_targets)\n",
    "        loss_error = loss_error + (1.0 - e).sum().item()\n",
    "        # the number of examples\n",
    "        N = N + test_batch.shape[0]\n",
    "    # divide by the number of examples\n",
    "    loss_test = loss_test / N\n",
    "    loss_error = loss_error / N\n",
    "\n",
    "    # Print the performance\n",
    "    if epoch is None:\n",
    "        print(f\"-> FINAL PERFORMANCE: nll={loss_test}, ce={loss_error}\")\n",
    "    else:\n",
    "        if verbose and epoch % 2 == 0:\n",
    "            print(f\"Epoch: {epoch}, val nll={loss_test}, val ce={loss_error}\")\n",
    "\n",
    "    return loss_test, loss_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EA_for_NAS(object):\n",
    "    def __init__(self, pop_size, split_indexes, k = 2, sp = 1.5, recombination_rate = 0.5, mutation_rate = 0.5, scaling_factor = 1.5):\n",
    "        self.pop_size = pop_size\n",
    "        # the indexes that are starting index of different representation areas\n",
    "        # in our case = [0,3,5,10,12,14]\n",
    "        #i.e. first coding part is [0:3], second is [3:5], ... , [12:14], [14:]\n",
    "        self.split_indexes = split_indexes\n",
    "        self.k = k\n",
    "        # selection pressure\n",
    "        self.sp = sp\n",
    "        # RECIMBINATION HYPERPARAMETERS\n",
    "        # recombination rate\n",
    "        self.recombination_rate = recombination_rate\n",
    "        # MUTATION HYPERPARAMETERS\n",
    "        # mutation rate\n",
    "        self.mutation_rate = mutation_rate\n",
    "        # scaling factor\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.model_class_error_dict = {}\n",
    "\n",
    "    def linear_ranking_selection(self, x_old, f_old):\n",
    "        # linear ranking selecting\n",
    "        # formula is the following:\n",
    "        # p_i = 1/n * (sp - (2sp-2)*rank_i/(n-1)) where sp is the selection pressure with value is in [1,2]\n",
    "        # and rank_i is the rank of the individual i\n",
    "        # first we calculate the rank\n",
    "        rank = np.argsort(f_old)\n",
    "        # sort parents based on their fitness\n",
    "        x_old = [x for _, x in sorted(enumerate(x_old), key=lambda pair: (-f_old[pair[0]], pair[0]))]\n",
    "        # then we calculate the selection probability\n",
    "        n = len(x_old)\n",
    "        selection_probability = 1/n * (self.sp - (2*self.sp-2)*rank/(n-1))\n",
    "        # now create a line with the selection probability spaces \n",
    "        selection_line = np.zeros(n)\n",
    "        for i in range(n):\n",
    "            selection_line[i] = np.sum(selection_probability[:i])\n",
    "        # now we choose the parents based on the selection probability line\n",
    "        x_parents = np.zeros((n, len(x_old[0])))\n",
    "        f_parents = np.zeros((n,))\n",
    "        for i in range(n):\n",
    "            # select a random number between 0 and 1 uniformly\n",
    "            rand = np.random.uniform(0,1)\n",
    "            # go over each element in the selection line from start to end\n",
    "            for j in range(n):\n",
    "                # if the random number is smaller than the selection line element then choose that parent\n",
    "                if rand < selection_line[j]:\n",
    "                    x_parents[i] = x_old[j]\n",
    "                    f_parents[i] = f_old[j]\n",
    "                    break\n",
    "                if j == n-1:\n",
    "                    # if not greater then any previous element then choose the last parent\n",
    "                    x_parents[i] = x_old[n-1]\n",
    "                    f_parents[i] = f_old[n-1] \n",
    "                        \n",
    "        return x_parents, f_parents\n",
    "\n",
    "    def parent_selection(self, x_old, f_old):\n",
    "        # linear ranking selection\n",
    "        x_parents, f_parents = self.linear_ranking_selection(x_old, f_old)\n",
    "\n",
    "        return x_parents, f_parents\n",
    "\n",
    "    def one_point_crossover(self, x1, x2):\n",
    "        # choose a crossover point based on split_indexes \n",
    "        # only divide the set from split indexes so that the rules for binary representation will apply\n",
    "        idx = int(np.random.choice(self.split_indexes, size=1))\n",
    "        while idx == 0:\n",
    "            idx = int(np.random.choice(self.split_indexes, size=1))\n",
    "        # do crossover from index\n",
    "        x1[idx:], x2[idx:] = x2[idx:].copy(), x1[idx:].copy()\n",
    "        # return the crossovered versions\n",
    "        return x1, x2\n",
    "\n",
    "    \n",
    "    def n_point_crossover(self, x1, x2, n):\n",
    "        if n == 'uniform':\n",
    "            # this is just to be sure that they are always the same length (although this is the case in our case just to be sure)\n",
    "            n = len(self.split_indexes)\n",
    "        else:\n",
    "            # check if n is smaller than split points\n",
    "            n = int(n)\n",
    "            if n > len(self.split_indexes):\n",
    "                # if so, then set n to the length of the parents\n",
    "                n = len(self.split_indexes)\n",
    "                \n",
    "        # choose n crossover points\n",
    "        crossover_points = np.random.choice(self.split_indexes, n)\n",
    "        # sort the crossover points\n",
    "        crossover_points.sort()\n",
    "        # do crossover from index\n",
    "        if n % 2 == 0:\n",
    "            for i in range(0, n-1, 2):\n",
    "                x1[crossover_points[i]:crossover_points[i+1]], x2[crossover_points[i]:crossover_points[i+1]] = x2[crossover_points[i]:crossover_points[i+1]].copy(), x1[crossover_points[i]:crossover_points[i+1]].copy()\n",
    "        else:\n",
    "            for i in range(0, n-2, 2):\n",
    "                x1[crossover_points[i]:crossover_points[i+1]], x2[crossover_points[i]:crossover_points[i+1]] = x2[crossover_points[i]:crossover_points[i+1]].copy(), x1[crossover_points[i]:crossover_points[i+1]].copy()\n",
    "\n",
    "        # return the crossovered versions\n",
    "        return x1, x2\n",
    "\n",
    "    def recombination(self, x_parents, f_parents):\n",
    "        # first copy the parents into the children\n",
    "        x_children = x_parents.copy()\n",
    "        # for population size, choose if there will be a crossover or not\n",
    "        for i in range(self.pop_size):\n",
    "            # if there is a crossover\n",
    "            if np.random.rand() < self.recombination_rate:\n",
    "                # choose the parents to perform crossover \n",
    "                p1, p2 = np.random.choice(self.pop_size, 2, replace=False)\n",
    "                # choose the crossover type: 0.5 -> one point, 0.25 -> two point, 0.20 -> 3 point, 0.05 -> uniform\n",
    "                crossover_type = np.random.rand()\n",
    "                \n",
    "                if crossover_type < 0.5:\n",
    "                    x_children[p1], x_children[p2] = self.one_point_crossover(x_parents[p1], x_parents[p2])\n",
    "                elif crossover_type < 0.75:\n",
    "                    x_children[p1], x_children[p2] = self.n_point_crossover(x_parents[p1], x_parents[p2], n='2')\n",
    "                elif crossover_type < 0.95:\n",
    "                    x_children[p1], x_children[p2] = self.n_point_crossover(x_parents[p1], x_parents[p2], n='3')\n",
    "                else:\n",
    "                    x_children[p1], x_children[p2] = self.n_point_crossover(x_parents[p1], x_parents[p2], n='uniform')\n",
    "                \n",
    "        return x_children\n",
    "\n",
    "    def mutation(self, x_children):\n",
    "        # for each child, choose if there will be a mutation or not\n",
    "        for i in range(self.pop_size):\n",
    "            # if there is a mutation\n",
    "            child = x_children[i]\n",
    "            if np.random.rand() < self.mutation_rate:\n",
    "                # for each of the split points, choose if there will be a mutation or not\n",
    "                for j in range(len(self.split_indexes)-1):\n",
    "                    # if there is a mutation on that part\n",
    "                    if np.random.rand() < self.mutation_rate:\n",
    "                        # find where the bit has value 1 \n",
    "                        idx = np.where(child[self.split_indexes[j]:self.split_indexes[j+1]] == 1)[0] + self.split_indexes[j]\n",
    "                        # find the new index of the bit that will have value 1\n",
    "                        new_idx = int(np.random.choice(np.arange(self.split_indexes[j], self.split_indexes[j+1]), size=1))\n",
    "                        # if the new index is the same as the old one, then not mutation will occur\n",
    "                        #print('Search between {} and {}'.format(self.split_indexes[j], self.split_indexes[j+1]))\n",
    "                        #print('Found 1 at index {}'.format(idx))\n",
    "                        #print('New index is {}'.format(new_idx))\n",
    "                        child[idx] = 0\n",
    "                        child[new_idx] = 1\n",
    "                # for the last part\n",
    "                if np.random.rand() < self.mutation_rate:\n",
    "                    # find where the bit has value 1 \n",
    "                    idx = np.where(child[self.split_indexes[-1]:] == 1)[0]+ self.split_indexes[-1]\n",
    "                    # find the new index of the bit that will have value 1\n",
    "                    new_idx = int(np.random.choice(np.arange(self.split_indexes[-1], len(child)), size=1))\n",
    "                    # if the new index is the same as the old one, then not mutation will occur\n",
    "                    child[idx] = 0\n",
    "                    child[new_idx] = 1\n",
    "        return x_children\n",
    "\n",
    "    def survivor_selection(self, x_old, x_children, f_old, f_children):\n",
    "        # elitism selection\n",
    "        # first select the best k individuals from the old population\n",
    "        x_old_best = [x for _, x in sorted(enumerate(x_old), key=lambda pair: (-f_old[pair[0]], pair[0]))][:self.k]\n",
    "        f_old_best = sorted(f_old, reverse=True)[:self.k]\n",
    "        # drop the selected x_old_best and corresponding f_old_best individuals from the old population \n",
    "        x_old =  [x for _, x in sorted(enumerate(x_old), key=lambda pair: (-f_old[pair[0]], pair[0]))][self.k:]\n",
    "        f_old = sorted(f_old, reverse=True)[self.k:]\n",
    "        # now (μ, λ) selection \n",
    "        # first sort the children based on their fitness\n",
    "        x_children =[x for _, x in sorted(enumerate(x_children), key=lambda pair: (-f_children[pair[0]], pair[0]))]\n",
    "        f_children = sorted(f_children, reverse=True)\n",
    "        # now select the rest of the population based on the fitness only on children\n",
    "        # first let us make sure that there is enough children to fill the population\n",
    "        if self.pop_size - self.k > len(x_children):\n",
    "            # if not, apply (μ + λ) selection\n",
    "            # this is only to make sure that the code is robust\n",
    "            x = np.concatenate((x_old, x_children))\n",
    "            f = np.concatenate((f_old, f_children))\n",
    "            # sort the new population based on their fitness\n",
    "            x =  [x for _, x in sorted(enumerate(x), key=lambda pair: (-f[pair[0]], pair[0]))]\n",
    "            f =  sorted(f, reverse=True)\n",
    "            # now select the best individuals\n",
    "            x = x[:self.pop_size]\n",
    "            f = f[:self.pop_size]\n",
    "        else:\n",
    "            # apply (μ, λ) selection\n",
    "            x = np.concatenate((x_old_best, x_children[:self.pop_size-self.k]))\n",
    "            f = np.concatenate((f_old_best, f_children[:self.pop_size-self.k]))\n",
    "\n",
    "        return x, f\n",
    "    \n",
    "\n",
    "    def evaluate(self, x, max_patience = 0, num_epochs = 10, lr = 0.001, wd = 0.0001, k = 10, result_dir = 'results/', name = 'test', split_indexes = [0,3,5,10,12,14], lambda_fitness = 0.01):\n",
    "        f = []\n",
    "        for config in x:\n",
    "            classnet, number_of_weights = architecture_decoder(config)\n",
    "            name = ''.join(str(config))\n",
    "            # train the network\n",
    "            model = ClassifierNeuralNet(classnet)\n",
    "            optimizer = torch.optim.Adamax(\n",
    "                [p for p in model.parameters() if p.requires_grad == True],\n",
    "                lr=lr,\n",
    "                weight_decay=wd,\n",
    "            )\n",
    "\n",
    "            # Training procedure\n",
    "            model, error_val = training(\n",
    "                name=result_dir + name,\n",
    "                max_patience=max_patience,\n",
    "                num_epochs=num_epochs,\n",
    "                model=model,\n",
    "                optimizer=optimizer,\n",
    "                training_loader=training_loader,\n",
    "                val_loader=val_loader,\n",
    "            )\n",
    "            # calculate the fitness\n",
    "            # Objective=ClassError+λ 𝑁𝑝 /𝑁𝑚𝑎𝑥\n",
    "            # where 𝑁𝑝 denotes the number of weights of a model and 𝑁𝑚𝑎𝑥 \n",
    "            # denotes the number of weights of the largest model in the population\n",
    "            fitness = error_val + lambda_fitness * (number_of_weights/ Nmax)\n",
    "            f.append(fitness)\n",
    "            # also to track of each model, save the model and its errors to the main class\n",
    "            self.model_class_error_dict[name] = {'model': model, 'error_val': error_val}\n",
    "        return f\n",
    "\n",
    "  \n",
    "    def step(self, x_old, f_old):\n",
    "        # -------\n",
    "        # PLEASE FILL IN\n",
    "        # NOTE: This function must return x, f\n",
    "        # where x - population\n",
    "        #       f - fitness values of the population\n",
    "        # -------\n",
    "        #print('Initial population: ', x_old)\n",
    "\n",
    "        x_parents, f_parents = self.parent_selection(x_old, f_old)\n",
    "        \n",
    "        #print('Parents: ', x_parents)\n",
    "        x_children = self.recombination(x_parents, f_parents)\n",
    "        #print('Children: ', x_children)\n",
    "\n",
    "        x_children = self.mutation(x_children)\n",
    "        #print('Mutated children: ', x_children)\n",
    "\n",
    "        f_children = self.evaluate(x_children)\n",
    "\n",
    "        x, f = self.survivor_selection(x_old, x_children, f_old, f_children)\n",
    "\n",
    "        return x, f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gen = 100\n",
    "pop_size = 15\n",
    "num_epochs = 10\n",
    "result_dir = 'results/'\n",
    "name = 'test'\n",
    "split_indexes = [0,3,5,10,12,14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/iremdemir/opt/anaconda3/envs/comp411/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type ClassifierNeuralNet. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/iremdemir/opt/anaconda3/envs/comp411/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Reshape. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/Users/iremdemir/opt/anaconda3/envs/comp411/lib/python3.7/site-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type Flatten. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation:  0 Best fitness:  0.03571428571428571\n",
      "Generation:  1 Best fitness:  0.03571428571428571\n",
      "Generation:  2 Best fitness:  0.03857142857142857\n",
      "Generation:  3 Best fitness:  0.03571428571428571\n",
      "Generation:  4 Best fitness:  0.03571428571428571\n",
      "Generation:  5 Best fitness:  0.04428571428571429\n",
      "Generation:  6 Best fitness:  0.03571428571428571\n",
      "Generation:  7 Best fitness:  0.03\n",
      "Generation:  8 Best fitness:  0.04142857142857143\n",
      "Generation:  9 Best fitness:  0.032857142857142856\n",
      "Generation:  10 Best fitness:  0.03571428571428571\n",
      "Generation:  11 Best fitness:  0.03857142857142857\n",
      "Generation:  12 Best fitness:  0.04142857142857143\n",
      "Generation:  13 Best fitness:  0.03857142857142857\n",
      "Generation:  14 Best fitness:  0.03571428571428571\n",
      "Generation:  15 Best fitness:  0.03857142857142857\n",
      "Generation:  16 Best fitness:  0.03857142857142857\n",
      "Generation:  17 Best fitness:  0.03857142857142857\n",
      "Generation:  18 Best fitness:  0.04142857142857143\n",
      "Generation:  19 Best fitness:  0.04142857142857143\n",
      "Generation:  20 Best fitness:  0.03571428571428571\n",
      "Generation:  21 Best fitness:  0.032857142857142856\n",
      "Generation:  22 Best fitness:  0.03571428571428571\n",
      "Generation:  23 Best fitness:  0.04142857142857143\n",
      "Generation:  24 Best fitness:  0.03857142857142857\n",
      "Generation:  25 Best fitness:  0.03\n",
      "Generation:  26 Best fitness:  0.03857142857142857\n",
      "Generation:  27 Best fitness:  0.03857142857142857\n",
      "Generation:  28 Best fitness:  0.03857142857142857\n",
      "Generation:  29 Best fitness:  0.03571428571428571\n",
      "Generation:  30 Best fitness:  0.04142857142857143\n",
      "Generation:  31 Best fitness:  0.04428571428571429\n",
      "Generation:  32 Best fitness:  0.03571428571428571\n",
      "Generation:  33 Best fitness:  0.03857142857142857\n",
      "Generation:  34 Best fitness:  0.03571428571428571\n",
      "Generation:  35 Best fitness:  0.03571428571428571\n",
      "Generation:  36 Best fitness:  0.04142857142857143\n",
      "Generation:  37 Best fitness:  0.03857142857142857\n",
      "Generation:  38 Best fitness:  0.03857142857142857\n",
      "Generation:  39 Best fitness:  0.03571428571428571\n",
      "Generation:  40 Best fitness:  0.03857142857142857\n",
      "Generation:  41 Best fitness:  0.04142857142857143\n",
      "Generation:  42 Best fitness:  0.03571428571428571\n",
      "Generation:  43 Best fitness:  0.03857142857142857\n",
      "Generation:  44 Best fitness:  0.03571428571428571\n",
      "Generation:  45 Best fitness:  0.03571428571428571\n",
      "Generation:  46 Best fitness:  0.03571428571428571\n",
      "Generation:  47 Best fitness:  0.03571428571428571\n",
      "Generation:  48 Best fitness:  0.032857142857142856\n",
      "Generation:  49 Best fitness:  0.03857142857142857\n",
      "Generation:  50 Best fitness:  0.032857142857142856\n",
      "Generation:  51 Best fitness:  0.03571428571428571\n",
      "Generation:  52 Best fitness:  0.03571428571428571\n",
      "Generation:  53 Best fitness:  0.04142857142857143\n",
      "Generation:  54 Best fitness:  0.03857142857142857\n",
      "Generation:  55 Best fitness:  0.03571428571428571\n",
      "Generation:  56 Best fitness:  0.03571428571428571\n",
      "Generation:  57 Best fitness:  0.03571428571428571\n",
      "Generation:  58 Best fitness:  0.03857142857142857\n",
      "Generation:  59 Best fitness:  0.03857142857142857\n",
      "Generation:  60 Best fitness:  0.03571428571428571\n",
      "Generation:  61 Best fitness:  0.03857142857142857\n",
      "Generation:  62 Best fitness:  0.03571428571428571\n",
      "Generation:  63 Best fitness:  0.03857142857142857\n",
      "Generation:  64 Best fitness:  0.03857142857142857\n",
      "Generation:  65 Best fitness:  0.04142857142857143\n",
      "Generation:  66 Best fitness:  0.03571428571428571\n",
      "Generation:  67 Best fitness:  0.03857142857142857\n",
      "Generation:  68 Best fitness:  0.03571428571428571\n",
      "Generation:  69 Best fitness:  0.04142857142857143\n",
      "Generation:  70 Best fitness:  0.03571428571428571\n",
      "Generation:  71 Best fitness:  0.032857142857142856\n",
      "Generation:  72 Best fitness:  0.03571428571428571\n",
      "Generation:  73 Best fitness:  0.03857142857142857\n",
      "Generation:  74 Best fitness:  0.032857142857142856\n",
      "Generation:  75 Best fitness:  0.03857142857142857\n",
      "Generation:  76 Best fitness:  0.03857142857142857\n",
      "Generation:  77 Best fitness:  0.03571428571428571\n",
      "Generation:  78 Best fitness:  0.03571428571428571\n",
      "Generation:  79 Best fitness:  0.03\n",
      "Generation:  80 Best fitness:  0.03571428571428571\n",
      "Generation:  81 Best fitness:  0.03571428571428571\n",
      "Generation:  82 Best fitness:  0.032857142857142856\n",
      "Generation:  83 Best fitness:  0.04142857142857143\n",
      "Generation:  84 Best fitness:  0.03857142857142857\n",
      "Generation:  85 Best fitness:  0.03857142857142857\n",
      "Generation:  86 Best fitness:  0.03857142857142857\n",
      "Generation:  87 Best fitness:  0.04428571428571429\n",
      "Generation:  88 Best fitness:  0.03571428571428571\n",
      "Generation:  89 Best fitness:  0.03857142857142857\n",
      "Generation:  90 Best fitness:  0.03857142857142857\n",
      "Generation:  91 Best fitness:  0.03857142857142857\n",
      "Generation:  92 Best fitness:  0.03857142857142857\n",
      "Generation:  93 Best fitness:  0.03857142857142857\n",
      "Generation:  94 Best fitness:  0.03571428571428571\n",
      "Generation:  95 Best fitness:  0.03857142857142857\n",
      "Generation:  96 Best fitness:  0.03571428571428571\n",
      "Generation:  97 Best fitness:  0.03571428571428571\n",
      "Generation:  98 Best fitness:  0.03857142857142857\n",
      "Generation:  99 Best fitness:  0.03571428571428571\n"
     ]
    }
   ],
   "source": [
    "# Initialize the EA class\n",
    "ea = EA_for_NAS(pop_size, split_indexes, k = 1, sp = 1.5, recombination_rate = 0.5, mutation_rate = 0.4, scaling_factor = 1.5)\n",
    "\n",
    "# Initialize the population\n",
    "x = init_generation_creator(pop_size, split_indexes, 24)\n",
    "f = ea.evaluate(x)\n",
    "\n",
    "# Evolutionary loop\n",
    "for i in range(max_gen):\n",
    "    x, f = ea.step(x, f)\n",
    "    print('Generation: ', i, 'Best fitness: ', np.min(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e-02, 2.28571429e-02, 2.57142857e-02, 2.85714286e-02,\n",
       "       3.14285714e-02, 3.42857143e-02, 3.71428571e-02, 4.00000000e-02,\n",
       "       4.28571429e-02, 4.57142857e-02, 4.85714286e-02, 5.14285714e-02,\n",
       "       5.71428571e-02, 6.28571429e-02, 1.11428571e-01, 1.14285714e-01,\n",
       "       1.17142857e-01, 1.20000000e-01, 1.22857143e-01, 1.25714286e-01,\n",
       "       1.28571429e-01, 1.31428571e-01, 1.34285714e-01, 1.37142857e-01,\n",
       "       1.40000000e-01, 1.42857143e-01, 1.45714286e-01, 1.48571429e-01,\n",
       "       1.51428571e-01, 1.54285714e-01, 1.71428571e-01, 2.08571429e-01,\n",
       "       2.11428571e-01, 2.14285714e-01, 2.17142857e-01, 2.20000000e-01,\n",
       "       2.22857143e-01, 2.25714286e-01, 2.28571429e-01, 2.31428571e-01,\n",
       "       2.34285714e-01, 2.40000000e-01, 2.42857143e-01, 2.45714286e-01,\n",
       "       3.00000000e-01, 3.11428571e-01, 3.14285714e-01, 3.17142857e-01,\n",
       "       3.22857143e-01, 3.37142857e-01, 4.00000000e-01, 4.14285714e-01,\n",
       "       4.22857143e-01, 4.25714286e-01, 5.05714286e-01, 5.14285714e-01,\n",
       "       6.08571429e-01, 7.08571429e-01, 1.00000000e+03])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ea.model_class_error_dict\n",
    "# find unique values in the dict for error_val\n",
    "unique_error_vals = np.unique([v['error_val'] for k, v in ea.model_class_error_dict.items()])\n",
    "unique_error_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> FINAL PERFORMANCE: nll=0.024267899910075552, ce=0.08053691275167785\n",
      "Test error:  (0.024267899910075552, 0.08053691275167785)\n"
     ]
    }
   ],
   "source": [
    "# select the model with minimum error_val from the dict\n",
    "best_model = [v['model'] for k, v in ea.model_class_error_dict.items() if v['error_val'] == np.min(unique_error_vals)][0]\n",
    "# evaluate the best model on the test set\n",
    "test_error = evaluation(model_best= best_model, name='best_model_from_ea', test_loader= test_loader)\n",
    "print('Test error: ', test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('comp411')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c40dd6e9603a48eda36538272fd95ed3b36cd00a3b7df7195515789ffdf02208"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
